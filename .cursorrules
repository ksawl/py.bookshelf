Когда пишешь код, будь на 100% уверен, что он будет работать и ты ничего не сломал из существующего.

## Technologies

- FastAPI - бэкэнд на пайтоне.
- Langchain (+Langsmith Dashboard) - фреймворк для простых ЛЛМ-агентов.
- ollama - бесплатный wrapper для локального хостинга маленьких LLM моделей.
- Pinecone - бесплатный web-hosted сервис для векторного хранилища.
- SQLite - локальная база данных для хранения метаинформации о книгах и их индексах.
- uv (https://docs.astral.sh/uv/) - для гарантии того что на любом устройстве код будет работать одинаково, используются package manager с lock файлом. Много лет в Python лидировал poetry, но уже полгода как оптимизированный аналог Uv занял нишу.
- Рекомендуемая версия Python: >=3.10,<=3.12.

## Functional requirements

1. Как API пользователь, я могу добавлять текстовые документы на книжную полку (выбрать 1 подходящий формат, которые поддерживает страницы и заголовки: docx/odt/google document/etc)

При добавлении документов, проводятся следующие действия:

- документ трансформируется в формат Markdown (наиболее читаемый формат для LLM)
- заголовки извлекаются из документа
- документ разбивается на chunks ~одинакового размера с overlap 20%, chunks индексируются в Pinecone индексе (1 документ = 1 индекс)
- к каждому chunk прикреплены метаданные: страница, заголовок (или цепь заголовков + подзаголовков)
- в начале каждого chunk прописывается заголовок + подзаголовки, к которому он принадлежит

2. Как API пользователь, я могу задавать вопросы по ранее добавленным документам и получать релевантные ответы с приведенным ссылками на источник (глава, страница)

При формировании ответа, выполняются следующие действия:

- с помощью ЛЛМ (self-hosted на ollama, обращение - через Langchain), вопрос клиента переписывается как короткий запрос оптимальный для векторного хранилища
- запрос отправляется в Pinecone, полученные в ответ chunks сортируются по своей релевантности
- формируется запрос в LLM:
  1.  Инструкция о задаче ассистента отвечать на вопрос вникая в смысл документа и использовать информацию исключительно из документа, не додумывая ничего стороннего.
  2.  Top 5 chunks
  3.  Иначальный запрос пользователя
- полученный от LLM ответ возвращается пользователю, с упоминанием использованных chunks как ссылки на источники.

3. Как API пользователь, я могу удалять ранее добавленные документы.

## API endpoints

GET /bookshelf - получить список индексированных книг и их внутреннего id.

POST /bookshelf - добавить новую книгу (отправить файл). Книга отправляется на индексацию в Pinecone. Одна книга = один индекс, а в бесплатной версии Pinecone разрешается только один индекс. Данный endpoint сработает только после предварительного очищения bookshelf. Возвращает book_id.

GET /bookshelf/{book_id} - получить метаинформацию о книге.

POST /bookshelf/{book_id} - задать вопрос по книге, получить ответ и перечень ссылок на источник.

DELETE /bookshelf/{book_id} - удалить книгу и очистить связанный с ней индекс.
